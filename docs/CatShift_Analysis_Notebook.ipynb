{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hey, That's My Data! - A Jupyter Notebook for CatShift Analysis\n",
    "\n",
    "This notebook provides a conceptual, step-by-step implementation of the **CatShift** framework, based on the paper: *Hey, That's My Data! Label-Only Dataset Inference in Large Language Models* ([arxiv.org/pdf/2506.06057](https://arxiv.org/pdf/2506.06057)).\n",
    "\n",
    "The goal of CatShift is to determine if a Large Language Model (LLM) was trained on a specific \"suspect\" dataset, using only its public-facing API (label-only) and its fine-tuning capabilities.\n",
    "\n",
    "### How to Use This Notebook:\n",
    "1.  **Replace Placeholders:** The functions in the *Mock/Placeholder Functions* section need to be implemented with real code that calls your chosen LLM's API (e.g., OpenAI, Anthropic) or a local library (e.g., Hugging Face Transformers).\n",
    "2.  **Load Real Data:** Replace the mock dictionaries in the *Data Preparation* section with your actual suspect and validation datasets.\n",
    "3.  **Manage API Keys and Costs:** Be mindful that fine-tuning commercial models via an API can incur costs.\n",
    "\n",
    "This framework provides a strong foundation for conducting your own CatShift analysis to investigate potential dataset usage in large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, we import the necessary libraries. `numpy` is used for numerical operations, `scipy.stats.ks_2samp` for the statistical test, and `matplotlib` for visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For a real implementation, you would uncomment and use libraries like these:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mock/Placeholder Functions\n",
    "\n",
    "In a real-world scenario, these functions would interact with a live LLM. For this demonstration, they are placeholders that simulate the behavior described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_scorer():\n",
    "    \"\"\"\n",
    "    Placeholder for loading a pretrained LLM and a sentence similarity model.\n",
    "    \"\"\"\n",
    "    print(\"INFO: Loading base LLM and similarity scorer (conceptual)...\")\n",
    "    mock_model = \"base_model\"\n",
    "    mock_similarity_scorer = \"scorer\"\n",
    "    print(\"INFO: Models loaded.\")\n",
    "    return mock_model, mock_similarity_scorer\n",
    "\n",
    "def get_completions(model_id, dataset):\n",
    "    \"\"\"\n",
    "    Placeholder for generating top-1 text completions from a given model.\n",
    "    \"\"\"\n",
    "    print(f\"INFO: Generating completions from model '{model_id}'...\")\n",
    "    completions = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        prompt = item['prompt']\n",
    "        if \"base\" in model_id:\n",
    "            completion = f\"{prompt} ...and so on.\"\n",
    "        elif \"suspect\" in model_id and \"old book\" in prompt:\n",
    "            completion = f\"{prompt} The quick brown fox jumps over the lazy dog.\"\n",
    "        elif \"validation\" in model_id and \"new article\" in prompt:\n",
    "            completion = f\"{prompt} The latest discovery is astounding.\"\n",
    "        else:\n",
    "            completion = f\"{prompt} This is a new completion.\"\n",
    "        completions.append(completion)\n",
    "    return completions\n",
    "\n",
    "def fine_tune_model(base_model, training_dataset, new_model_id):\n",
    "    \"\"\"\n",
    "    Placeholder for the fine-tuning process.\n",
    "    \"\"\"\n",
    "    print(f\"INFO: Fine-tuning '{base_model}' on {len(training_dataset)} samples to create '{new_model_id}'...\")\n",
    "    print(f\"INFO: Fine-tuning complete. New model ID is '{new_model_id}'.\")\n",
    "    return new_model_id\n",
    "\n",
    "def compute_similarity_scores(completions_before, completions_after, scorer):\n",
    "    \"\"\"\n",
    "    Placeholder for computing similarity between two lists of text.\n",
    "    \"\"\"\n",
    "    print(\"INFO: Computing similarity scores...\")\n",
    "    scores = [np.random.uniform(low=0.7, high=0.95) for _ in completions_before]\n",
    "    # Heuristically simulate a larger output shift (lower similarity) for member data\n",
    "    if \"The quick brown fox\" in completions_after[0]: \n",
    "      scores = [s - np.random.uniform(0.3, 0.5) for s in scores]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The CatShift Analysis Pipeline\n",
    "\n",
    "This function executes the core logic of CatShift: generate, fine-tune, generate again, and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_catshift_analysis(base_model, scorer, suspect_dataset, validation_dataset):\n",
    "    \"\"\"\n",
    "    Executes the full CatShift pipeline on a suspect dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting CatShift Analysis for SUSPECT Dataset ---\")\n",
    "    completions_before_suspect = get_completions(base_model, suspect_dataset['test'])\n",
    "    finetuned_model_suspect = fine_tune_model(base_model, suspect_dataset['train'], 'finetuned_model_suspect')\n",
    "    completions_after_suspect = get_completions(finetuned_model_suspect, suspect_dataset['test'])\n",
    "    suspect_scores = compute_similarity_scores(completions_before_suspect, completions_after_suspect, scorer)\n",
    "    print(f\"SUSPECT SET: Average similarity score = {np.mean(suspect_scores):.4f}\")\n",
    "\n",
    "    print(\"\\n--- Establishing Baseline using VALIDATION Dataset ---\")\n",
    "    completions_before_validation = get_completions(base_model, validation_dataset['test'])\n",
    "    finetuned_model_validation = fine_tune_model(base_model, validation_dataset['train'], 'finetuned_model_validation')\n",
    "    completions_after_validation = get_completions(finetuned_model_validation, validation_dataset['test'])\n",
    "    validation_scores = compute_similarity_scores(completions_before_validation, completions_after_validation, scorer)\n",
    "    print(f\"VALIDATION SET: Average similarity score = {np.mean(validation_scores):.4f}\")\n",
    "\n",
    "    print(\"\\n--- HYPOTHESIS TESTING ---\")\n",
    "    statistic, p_value = ks_2samp(suspect_scores, validation_scores)\n",
    "    print(f\"KS Statistic: {statistic:.4f}\")\n",
    "    print(f\"P-value: {p_value:.6f}\")\n",
    "    \n",
    "    return p_value, suspect_scores, validation_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "Here, we set up our mock datasets. \n",
    "- The `suspect_member_dataset` simulates data that the LLM *was* trained on.\n",
    "- The `known_non_member_dataset` simulates data that the LLM has *never* seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset simulates data that the LLM *was* trained on.\n",
    "# The fine-tuning process will cause a \"recovery\" of forgotten knowledge,\n",
    "# leading to a large output shift (and thus low similarity scores).\n",
    "suspect_member_dataset = {\n",
    "    'train': [{'prompt': f'From the old book, chapter {i}:'} for i in range(50)],\n",
    "    'test': [{'prompt': f'From the old book, chapter {i+50}:'} for i in range(20)]\n",
    "}\n",
    "\n",
    "# This dataset simulates data that the LLM was *not* trained on.\n",
    "# The fine-tuning process introduces entirely new knowledge,\n",
    "# leading to a smaller, more moderate output shift.\n",
    "known_non_member_dataset = {\n",
    "    'train': [{'prompt': f'From the new article, section {i}:'} for i in range(50)],\n",
    "    'test': [{'prompt': f'From the new article, section {i+50}:'} for i in range(20)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execution and Interpretation\n",
    "\n",
    "Now, we run the full analysis and interpret the results based on the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models (conceptually)\n",
    "base_llm, similarity_scorer = get_model_and_scorer()\n",
    "\n",
    "# Run the analysis\n",
    "p_value_result, suspect_scores, validation_scores = run_catshift_analysis(\n",
    "    base_llm, \n",
    "    similarity_scorer, \n",
    "    suspect_member_dataset, \n",
    "    known_non_member_dataset\n",
    ")\n",
    "\n",
    "# --- INTERPRETATION ---\n",
    "# The paper uses a significance level (alpha) of 0.1\n",
    "alpha = 0.1\n",
    "print(\"\\n--- FINAL CONCLUSION ---\")\n",
    "if p_value_result < alpha:\n",
    "    print(f\"Result: P-value ({p_value_result:.6f}) is less than alpha ({alpha}).\")\n",
    "    print(\"Conclusion: We REJECT the null hypothesis.\")\n",
    "    print(\"This suggests the suspect dataset's output shift is SIGNIFICANTLY DIFFERENT from the non-member baseline.\")\n",
    "    print(\"Therefore, the suspect dataset is LIKELY a MEMBER of the original training data.\")\n",
    "else:\n",
    "    print(f\"Result: P-value ({p_value_result:.6f}) is not less than alpha ({alpha}).\")\n",
    "    print(\"Conclusion: We FAIL to reject the null hypothesis.\")\n",
    "    print(\"There is no strong statistical evidence to distinguish the suspect dataset from a non-member.\")\n",
    "    print(\"Therefore, the suspect dataset is LIKELY NOT a member of the original training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "A key part of data analysis is visualization. Let's plot histograms of the two similarity score distributions. We expect to see a clear separation between the two.\n",
    "\n",
    "- **Validation Scores (Blue)**: Should be higher, indicating less change after fine-tuning.\n",
    "- **Suspect Scores (Red)**: Should be lower, indicating a significant change (a \"shift\") as the model \"recovers\" forgotten knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(validation_scores, bins=15, alpha=0.7, label='Validation (Non-Member) Scores', color='blue', density=True)\n",
    "plt.hist(suspect_scores, bins=15, alpha=0.7, label='Suspect (Member) Scores', color='red', density=True)\n",
    "\n",
    "plt.title('Distribution of Similarity Scores Before vs. After Fine-Tuning')\n",
    "plt.xlabel('Similarity Score (Lower score = bigger output shift)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
