{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "def preprocess_data(filepath, overall_satisfaction_col, feature_cols, \n",
    "                    dissatisfaction_threshold, failure_threshold_map,\n",
    "                    overall_score_higher_is_better=True, \n",
    "                    feature_score_higher_is_better=True):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the data.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the CSV file.\n",
    "        overall_satisfaction_col (str): Name of the overall satisfaction column.\n",
    "        feature_cols (list): List of feature column names.\n",
    "        dissatisfaction_threshold (float): Threshold for overall satisfaction.\n",
    "                                           Scores <= threshold are \"Dissatisfied\" if higher_is_better=True,\n",
    "                                           Scores >= threshold are \"Dissatisfied\" if higher_is_better=False.\n",
    "        failure_threshold_map (dict): A dictionary mapping feature_col_name to its failure threshold.\n",
    "                                      Scores <= threshold are \"Failed\" if higher_is_better=True for that feature,\n",
    "                                      Scores >= threshold are \"Failed\" if higher_is_better=False for that feature.\n",
    "        overall_score_higher_is_better (bool): True if higher overall scores mean more satisfaction.\n",
    "        feature_score_higher_is_better (bool): True if higher feature scores mean better performance.\n",
    "                                               Can be overridden per feature in failure_threshold_map if needed\n",
    "                                               by making threshold_map values tuples: (threshold, higher_is_better_for_feature)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with binarized columns.\n",
    "        str: Name of the binarized overall satisfaction column.\n",
    "        list: List of binarized feature column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Binarize overall satisfaction\n",
    "    binarized_overall_col = f\"{overall_satisfaction_col}_Dissatisfied\"\n",
    "    if overall_score_higher_is_better:\n",
    "        df[binarized_overall_col] = (df[overall_satisfaction_col] <= dissatisfaction_threshold).astype(int)\n",
    "    else:\n",
    "        df[binarized_overall_col] = (df[overall_satisfaction_col] >= dissatisfaction_threshold).astype(int)\n",
    "\n",
    "    # Binarize feature columns\n",
    "    binarized_feature_cols = []\n",
    "    for col in feature_cols:\n",
    "        bin_col_name = f\"{col}_Failed\"\n",
    "        threshold_info = failure_threshold_map.get(col)\n",
    "        \n",
    "        current_feature_higher_is_better = feature_score_higher_is_better # Default\n",
    "        current_failure_threshold = None\n",
    "\n",
    "        if isinstance(threshold_info, tuple): # (threshold, specific_higher_is_better)\n",
    "            current_failure_threshold = threshold_info[0]\n",
    "            current_feature_higher_is_better = threshold_info[1]\n",
    "        elif threshold_info is not None: # Just threshold, use global feature_score_higher_is_better\n",
    "            current_failure_threshold = threshold_info\n",
    "        else:\n",
    "            raise ValueError(f\"Failure threshold not defined for feature: {col}\")\n",
    "\n",
    "        if current_feature_higher_is_better:\n",
    "            df[bin_col_name] = (df[col] <= current_failure_threshold).astype(int)\n",
    "        else:\n",
    "            df[bin_col_name] = (df[col] >= current_failure_threshold).astype(int)\n",
    "        binarized_feature_cols.append(bin_col_name)\n",
    "        \n",
    "    return df, binarized_overall_col, binarized_feature_cols\n",
    "\n",
    "def get_value_of_coalition(df, coalition_bin_feature_cols, binarized_overall_col):\n",
    "    \"\"\"\n",
    "    Calculates the value v(M) = Reach_M - Noise_M for a given coalition M.\n",
    "    M is represented by a list of binarized feature column names.\n",
    "    \"\"\"\n",
    "    if not coalition_bin_feature_cols: # Empty coalition\n",
    "        return 0.0\n",
    "\n",
    "    # Mask for rows where at least one feature in the coalition has \"Failed\" (is 1)\n",
    "    failed_on_any_in_coalition_mask = df[coalition_bin_feature_cols].any(axis=1)\n",
    "    \n",
    "    df_dissatisfied = df[df[binarized_overall_col] == 1]\n",
    "    df_not_dissatisfied = df[df[binarized_overall_col] == 0]\n",
    "\n",
    "    num_total_dissatisfied = len(df_dissatisfied)\n",
    "    num_total_not_dissatisfied = len(df_not_dissatisfied)\n",
    "\n",
    "    if num_total_dissatisfied == 0 and num_total_not_dissatisfied == 0:\n",
    "        # This case should ideally not happen with reasonable data.\n",
    "        # If it does, it means no data points, so value is undefined or 0.\n",
    "        print(\"Warning: No data points found for calculating coalition value.\")\n",
    "        return 0.0\n",
    "    \n",
    "    # Reach_M = P(Failed_on_any_in_M | Dissatisfied)\n",
    "    # = Count(Failed_on_any_in_M AND Dissatisfied) / Count(Dissatisfied)\n",
    "    if num_total_dissatisfied > 0:\n",
    "        num_failed_and_dissatisfied = df_dissatisfied[failed_on_any_in_coalition_mask[df_dissatisfied.index]].shape[0]\n",
    "        reach_M = num_failed_and_dissatisfied / num_total_dissatisfied\n",
    "    else:\n",
    "        reach_M = 0.0 # No dissatisfied customers, so cannot be reached via failure.\n",
    "\n",
    "    # Noise_M = P(Failed_on_any_in_M | Not Dissatisfied)\n",
    "    # = Count(Failed_on_any_in_M AND Not Dissatisfied) / Count(Not Dissatisfied)\n",
    "    if num_total_not_dissatisfied > 0:\n",
    "        num_failed_and_not_dissatisfied = df_not_dissatisfied[failed_on_any_in_coalition_mask[df_not_dissatisfied.index]].shape[0]\n",
    "        noise_M = num_failed_and_not_dissatisfied / num_total_not_dissatisfied\n",
    "    else:\n",
    "        noise_M = 0.0 # No not_dissatisfied customers.\n",
    "\n",
    "    value_M = reach_M - noise_M\n",
    "    return value_M\n",
    "\n",
    "def calculate_shapley_values(df, binarized_feature_cols, binarized_overall_col):\n",
    "    \"\"\"\n",
    "    Calculates Shapley values for each feature.\n",
    "    \"\"\"\n",
    "    num_features = len(binarized_feature_cols)\n",
    "    feature_indices = list(range(num_features))\n",
    "    shapley_values = np.zeros(num_features)\n",
    "\n",
    "    for i in feature_indices: # For each feature 'k' (represented by index i)\n",
    "        feature_k_col_name = binarized_feature_cols[i]\n",
    "        \n",
    "        # Iterate over all possible coalitions M that DO NOT contain feature k\n",
    "        remaining_feature_indices = [idx for idx in feature_indices if idx != i]\n",
    "        \n",
    "        for m_size in range(num_features): # m_size is the size of coalition M (from 0 to n-1)\n",
    "            # gamma_n(M) = m! * (n - m - 1)! / n!\n",
    "            # Here, n is num_features, m is m_size\n",
    "            if num_features - m_size -1 < 0 : # Avoid factorial of negative\n",
    "                 # This case happens when m_size = num_features, which means coalition M includes all other n-1 features.\n",
    "                 # The marginal contribution is to the grand coalition.\n",
    "                 # The loop for m_size should go up to num_features -1 if we consider M not containing k.\n",
    "                 # If M is of size m, M U {k} is of size m+1.\n",
    "                 # The sum is over coalitions M of size m, where m ranges from 0 to n-1.\n",
    "                 # (n-m-1)! is (num_features - m_size -1)!\n",
    "                 # This gamma is for a coalition M of size m_size.\n",
    "                 pass # will be handled by itertools.combinations\n",
    "\n",
    "            gamma_weight = (math.factorial(m_size) * math.factorial(num_features - m_size - 1)) / math.factorial(num_features)\n",
    "\n",
    "            for M_indices_tuple in itertools.combinations(remaining_feature_indices, m_size):\n",
    "                M_cols = [binarized_feature_cols[idx] for idx in M_indices_tuple]\n",
    "                M_union_k_cols = M_cols + [feature_k_col_name]\n",
    "\n",
    "                v_M_union_k = get_value_of_coalition(df, M_union_k_cols, binarized_overall_col)\n",
    "                v_M = get_value_of_coalition(df, M_cols, binarized_overall_col)\n",
    "                \n",
    "                shapley_values[i] += gamma_weight * (v_M_union_k - v_M)\n",
    "                \n",
    "    return {binarized_feature_cols[i]: shapley_values[i] for i in range(num_features)}\n",
    "\n",
    "\n",
    "def get_reach_noise_success_for_set(df, set_bin_feature_cols, binarized_overall_col):\n",
    "    \"\"\"Calculates Reach, Noise, and Success for a given set of (binarized) features.\"\"\"\n",
    "    if not set_bin_feature_cols:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    failed_on_any_in_set_mask = df[set_bin_feature_cols].any(axis=1)\n",
    "    \n",
    "    df_dissatisfied = df[df[binarized_overall_col] == 1]\n",
    "    df_not_dissatisfied = df[df[binarized_overall_col] == 0]\n",
    "\n",
    "    num_total_dissatisfied = len(df_dissatisfied)\n",
    "    num_total_not_dissatisfied = len(df_not_dissatisfied)\n",
    "\n",
    "    if num_total_dissatisfied == 0 and num_total_not_dissatisfied == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    if num_total_dissatisfied > 0:\n",
    "        num_failed_and_dissatisfied = df_dissatisfied[failed_on_any_in_set_mask[df_dissatisfied.index]].shape[0]\n",
    "        reach = num_failed_and_dissatisfied / num_total_dissatisfied\n",
    "    else:\n",
    "        reach = 0.0\n",
    "\n",
    "    if num_total_not_dissatisfied > 0:\n",
    "        num_failed_and_not_dissatisfied = df_not_dissatisfied[failed_on_any_in_set_mask[df_not_dissatisfied.index]].shape[0]\n",
    "        noise = num_failed_and_not_dissatisfied / num_total_not_dissatisfied\n",
    "    else:\n",
    "        noise = 0.0\n",
    "        \n",
    "    success = reach - noise\n",
    "    return reach, noise, success\n",
    "\n",
    "\n",
    "def determine_key_drivers(df, binarized_feature_cols, binarized_overall_col, shapley_values_dict):\n",
    "    \"\"\"\n",
    "    Determines the key dissatisfiers based on Shapley values and the Success metric.\n",
    "    \"\"\"\n",
    "    # Sort features by Shapley value in descending order\n",
    "    sorted_features = sorted(shapley_values_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n--- Determining Key Dissatisfiers (Cumulative Analysis) ---\")\n",
    "    print(f\"{'Step':<5} {'Added Feature':<30} {'Cumulative Set Size':<20} {'Reach':<10} {'Noise':<10} {'Success':<10}\")\n",
    "    \n",
    "    cumulative_set_cols = []\n",
    "    optimal_set_cols = []\n",
    "    max_success_achieved = -float('inf')\n",
    "    \n",
    "    results_log = []\n",
    "\n",
    "    for i, (feature_col, sv) in enumerate(sorted_features):\n",
    "        current_cumulative_cols_for_step = cumulative_set_cols + [feature_col]\n",
    "        \n",
    "        reach, noise, success = get_reach_noise_success_for_set(df, current_cumulative_cols_for_step, binarized_overall_col)\n",
    "        \n",
    "        results_log.append({\n",
    "            'step': i + 1,\n",
    "            'added_feature': feature_col.replace('_Failed', ''),\n",
    "            'set_size': len(current_cumulative_cols_for_step),\n",
    "            'reach': reach,\n",
    "            'noise': noise,\n",
    "            'success': success,\n",
    "            'current_set_cols_for_step': list(current_cumulative_cols_for_step) # for debugging or internal use\n",
    "        })\n",
    "        \n",
    "        print(f\"{i+1:<5} {feature_col.replace('_Failed', ''):<30} {len(current_cumulative_cols_for_step):<20} {reach:<10.3f} {noise:<10.3f} {success:<10.3f}\")\n",
    "\n",
    "        if success >= max_success_achieved : # If current success is better or equal, update optimal set and continue\n",
    "            max_success_achieved = success\n",
    "            optimal_set_cols = list(current_cumulative_cols_for_step) # Keep this set as potentially optimal\n",
    "            cumulative_set_cols.append(feature_col) # Add to the set for the next iteration\n",
    "        else:\n",
    "            # Success decreased, so the set from the *previous* step was optimal.\n",
    "            # `optimal_set_cols` already holds the best set found so far that led to max_success_achieved.\n",
    "            print(f\"Success decreased. Optimal set identified before adding '{feature_col.replace('_Failed', '')}'.\")\n",
    "            break \n",
    "            \n",
    "    # If the loop completed without success decreasing, the last cumulative set is optimal.\n",
    "    # This is handled because optimal_set_cols is updated whenever success >= max_success_achieved.\n",
    "\n",
    "    return [col.replace('_Failed', '') for col in optimal_set_cols], results_log\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the analysis.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    # Replace with your actual file path\n",
    "    filepath = 'sample_customer_data.csv' \n",
    "    \n",
    "    # Define overall satisfaction column and its dissatisfaction threshold\n",
    "    overall_satisfaction_col = 'OverallSatisfaction' # Raw score column\n",
    "    # Assuming 1-10 scale, higher is better. Scores <= 5 are \"Dissatisfied\".\n",
    "    dissatisfaction_threshold = 5 \n",
    "    overall_score_higher_is_better = True\n",
    "\n",
    "    # Define feature columns and their failure thresholds\n",
    "    # Format: { 'feature_col_name': threshold } or \n",
    "    #         { 'feature_col_name': (threshold, higher_is_better_for_this_feature) }\n",
    "    feature_cols = ['FeatureA', 'FeatureB', 'FeatureC', 'FeatureD']\n",
    "    failure_threshold_map = {\n",
    "        'FeatureA': 3, # Assuming 1-5 scale, higher is better. Scores <=3 are \"Failed\".\n",
    "        'FeatureB': 3,\n",
    "        'FeatureC': (7, False), # Assuming 1-10 scale, but for this feature LOWER is better. Scores >=7 are \"Failed\".\n",
    "        'FeatureD': 2\n",
    "    }\n",
    "    # Default assumption for feature scores (can be overridden in failure_threshold_map)\n",
    "    feature_score_higher_is_better = True \n",
    "\n",
    "    # --- Create a dummy sample_customer_data.csv for testing ---\n",
    "    data = {\n",
    "        'OverallSatisfaction': [2, 8, 5, 10, 3, 6, 1, 9, 4, 7, 2, 5, 8, 3, 6, 10, 1, 4, 9, 7],\n",
    "        'FeatureA':            [1, 5, 3,  4, 2, 5, 1, 4, 2, 3, 1, 3, 5, 2, 4, 5, 1, 2, 5, 3], # Scale 1-5, higher better\n",
    "        'FeatureB':            [2, 4, 2,  5, 1, 3, 2, 5, 1, 4, 2, 2, 4, 1, 3, 5, 2, 1, 5, 4], # Scale 1-5, higher better\n",
    "        'FeatureC':            [8, 3, 6,  2, 9, 4, 10,1, 7, 5, 8, 6, 2, 9, 4, 1, 10,7, 3, 5], # Scale 1-10, LOWER better\n",
    "        'FeatureD':            [1, 3, 1,  3, 1, 2, 1, 3, 1, 2, 1, 1, 3, 1, 2, 3, 1, 1, 3, 2]  # Scale 1-3, higher better\n",
    "    }\n",
    "    df_sample = pd.DataFrame(data)\n",
    "    df_sample.to_csv(filepath, index=False)\n",
    "    print(f\"Created dummy data at {filepath}\")\n",
    "    # --- End of dummy data creation ---\n",
    "\n",
    "    # 1. Preprocess data\n",
    "    df_processed, bin_overall_col, bin_feature_cols = preprocess_data(\n",
    "        filepath,\n",
    "        overall_satisfaction_col,\n",
    "        feature_cols,\n",
    "        dissatisfaction_threshold,\n",
    "        failure_threshold_map,\n",
    "        overall_score_higher_is_better,\n",
    "        feature_score_higher_is_better\n",
    "    )\n",
    "    print(\"\\n--- Processed Data Head ---\")\n",
    "    print(df_processed[[bin_overall_col] + bin_feature_cols].head())\n",
    "\n",
    "    # Check if there are any dissatisfied or not dissatisfied customers\n",
    "    if df_processed[bin_overall_col].nunique() < 2:\n",
    "        print(f\"\\nWarning: The binarized overall satisfaction column '{bin_overall_col}' has only one unique value.\")\n",
    "        print(\"This will lead to Reach or Noise (or both) being undefined or zero for all coalitions.\")\n",
    "        print(\"Please check your dissatisfaction_threshold and data distribution.\")\n",
    "        # Depending on the case, you might want to exit or handle differently\n",
    "        if df_processed[df_processed[bin_overall_col] == 1].empty:\n",
    "            print(\"No customers are marked as 'Dissatisfied'.\")\n",
    "        if df_processed[df_processed[bin_overall_col] == 0].empty:\n",
    "            print(\"No customers are marked as 'Not Dissatisfied'.\")\n",
    "        # return # Optionally exit if data is unsuitable\n",
    "\n",
    "    # 2. Calculate Shapley values\n",
    "    print(\"\\nCalculating Shapley values... (this may take time for many features)\")\n",
    "    shapley_values = calculate_shapley_values(df_processed, bin_feature_cols, bin_overall_col)\n",
    "    \n",
    "    print(\"\\n--- Shapley Values ---\")\n",
    "    for feature, sv in sorted(shapley_values.items(), key=lambda item: item[1], reverse=True):\n",
    "        print(f\"{feature.replace('_Failed', ''):<30}: {sv:.4f}\")\n",
    "\n",
    "    # 3. Determine Key Dissatisfiers\n",
    "    key_dissatisfiers, full_log = determine_key_drivers(df_processed, bin_feature_cols, bin_overall_col, shapley_values)\n",
    "    \n",
    "    print(\"\\n--- Final Set of Key Dissatisfiers ---\")\n",
    "    if key_dissatisfiers:\n",
    "        for kd in key_dissatisfiers:\n",
    "            print(f\"- {kd}\")\n",
    "    else:\n",
    "        print(\"No key dissatisfiers identified based on the criteria (or no features provided).\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ],
   "id": "efc1272841cb7355"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
