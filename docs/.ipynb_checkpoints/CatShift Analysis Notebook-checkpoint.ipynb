{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"# Hey, That's My Data! - A Jupyter Notebook for CatShift Analysis\\n\",\n",
    "        \"\\n\",\n",
    "        \"This notebook provides a conceptual, step-by-step implementation of the **CatShift** framework, based on the paper: *Hey, That's My Data! Label-Only Dataset Inference in Large Language Models* ([arxiv.org/pdf/2506.06057](https://arxiv.org/pdf/2506.06057)).\\n\",\n",
    "        \"\\n\",\n",
    "        \"The goal of CatShift is to determine if a Large Language Model (LLM) was trained on a specific \\\"suspect\\\" dataset, using only its public-facing API (label-only) and its fine-tuning capabilities.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### How to Use This Notebook:\\n\",\n",
    "        \"1.  **Replace Placeholders:** The functions in the *Mock/Placeholder Functions* section need to be implemented with real code that calls your chosen LLM's API (e.g., OpenAI, Anthropic) or a local library (e.g., Hugging Face Transformers).\\n\",\n",
    "        \"2.  **Load Real Data:** Replace the mock dictionaries in the *Data Preparation* section with your actual suspect and validation datasets.\\n\",\n",
    "        \"3.  **Manage API Keys and Costs:** Be mindful that fine-tuning commercial models via an API can incur costs.\\n\",\n",
    "        \"\\n\",\n",
    "        \"This framework provides a strong foundation for conducting your own CatShift analysis to investigate potential dataset usage in large language models.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 1. Setup and Imports\\n\",\n",
    "        \"\\n\",\n",
    "        \"First, we import the necessary libraries. `numpy` is used for numerical operations, `scipy.stats.ks_2samp` for the statistical test, and `matplotlib` for visualizing the results.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import numpy as np\\n\",\n",
    "        \"from scipy.stats import ks_2samp\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"\\n\",\n",
    "        \"# For a real implementation, you would uncomment and use libraries like these:\\n\",\n",
    "        \"# from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\\n\",\n",
    "        \"# from sentence_transformers import SentenceTransformer, util\\n\",\n",
    "        \"# import openai\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 2. Mock/Placeholder Functions\\n\",\n",
    "        \"\\n\",\n",
    "        \"In a real-world scenario, these functions would interact with a live LLM. For this demonstration, they are placeholders that simulate the behavior described in the paper.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def get_model_and_scorer():\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    Placeholder for loading a pretrained LLM and a sentence similarity model.\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    print(\\\"INFO: Loading base LLM and similarity scorer (conceptual)...\\\")\\n\",\n",
    "        \"    mock_model = \\\"base_model\\\"\\n\",\n",
    "        \"    mock_similarity_scorer = \\\"scorer\\\"\\n\",\n",
    "        \"    print(\\\"INFO: Models loaded.\\\")\\n\",\n",
    "        \"    return mock_model, mock_similarity_scorer\\n\",\n",
    "        \"\\n\",\n",
    "        \"def get_completions(model_id, dataset):\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    Placeholder for generating top-1 text completions from a given model.\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    print(f\\\"INFO: Generating completions from model '{model_id}'...\\\")\\n\",\n",
    "        \"    completions = []\\n\",\n",
    "        \"    for i, item in enumerate(dataset):\\n\",\n",
    "        \"        prompt = item['prompt']\\n\",\n",
    "        \"        if \\\"base\\\" in model_id:\\n\",\n",
    "        \"            completion = f\\\"{prompt} ...and so on.\\\"\\n\",\n",
    "        \"        elif \\\"suspect\\\" in model_id and \\\"old book\\\" in prompt:\\n\",\n",
    "        \"            completion = f\\\"{prompt} The quick brown fox jumps over the lazy dog.\\\"\\n\",\n",
    "        \"        elif \\\"validation\\\" in model_id and \\\"new article\\\" in prompt:\\n\",\n",
    "        \"            completion = f\\\"{prompt} The latest discovery is astounding.\\\"\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            completion = f\\\"{prompt} This is a new completion.\\\"\\n\",\n",
    "        \"        completions.append(completion)\\n\",\n",
    "        \"    return completions\\n\",\n",
    "        \"\\n\",\n",
    "        \"def fine_tune_model(base_model, training_dataset, new_model_id):\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    Placeholder for the fine-tuning process.\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    print(f\\\"INFO: Fine-tuning '{base_model}' on {len(training_dataset)} samples to create '{new_model_id}'...\\\")\\n\",\n",
    "        \"    print(f\\\"INFO: Fine-tuning complete. New model ID is '{new_model_id}'.\\\")\\n\",\n",
    "        \"    return new_model_id\\n\",\n",
    "        \"\\n\",\n",
    "        \"def compute_similarity_scores(completions_before, completions_after, scorer):\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    Placeholder for computing similarity between two lists of text.\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    print(\\\"INFO: Computing similarity scores...\\\")\\n\",\n",
    "        \"    scores = [np.random.uniform(low=0.7, high=0.95) for _ in completions_before]\\n\",\n",
    "        \"    # Heuristically simulate a larger output shift (lower similarity) for member data\\n\",\n",
    "        \"    if \\\"The quick brown fox\\\" in completions_after[0]: \\n\",\n",
    "        \"      scores = [s - np.random.uniform(0.3, 0.5) for s in scores]\\n\",\n",
    "        \"    return scores\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 3. The CatShift Analysis Pipeline\\n\",\n",
    "        \"\\n\",\n",
    "        \"This function executes the core logic of CatShift: generate, fine-tune, generate again, and compare.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def run_catshift_analysis(base_model, scorer, suspect_dataset, validation_dataset):\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    Executes the full CatShift pipeline on a suspect dataset.\\n\",\n",
    "        \"    \\\"\\\"\\\"\\n\",\n",
    "        \"    print(\\\"\\\\n--- Starting CatShift Analysis for SUSPECT Dataset ---\\\")\\n\",\n",
    "        \"    completions_before_suspect = get_completions(base_model, suspect_dataset['test'])\\n\",\n",
    "        \"    finetuned_model_suspect = fine_tune_model(base_model, suspect_dataset['train'], 'finetuned_model_suspect')\\n\",\n",
    "        \"    completions_after_suspect = get_completions(finetuned_model_suspect, suspect_dataset['test'])\\n\",\n",
    "        \"    suspect_scores = compute_similarity_scores(completions_before_suspect, completions_after_suspect, scorer)\\n\",\n",
    "        \"    print(f\\\"SUSPECT SET: Average similarity score = {np.mean(suspect_scores):.4f}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"    print(\\\"\\\\n--- Establishing Baseline using VALIDATION Dataset ---\\\")\\n\",\n",
    "        \"    completions_before_validation = get_completions(base_model, validation_dataset['test'])\\n\",\n",
    "        \"    finetuned_model_validation = fine_tune_model(base_model, validation_dataset['train'], 'finetuned_model_validation')\\n\",\n",
    "        \"    completions_after_validation = get_completions(finetuned_model_validation, validation_dataset['test'])\\n\",\n",
    "        \"    validation_scores = compute_similarity_scores(completions_before_validation, completions_after_validation, scorer)\\n\",\n",
    "        \"    print(f\\\"VALIDATION SET: Average similarity score = {np.mean(validation_scores):.4f}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"    print(\\\"\\\\n--- HYPOTHESIS TESTING ---\\\")\\n\",\n",
    "        \"    statistic, p_value = ks_2samp(suspect_scores, validation_scores)\\n\",\n",
    "        \"    print(f\\\"KS Statistic: {statistic:.4f}\\\")\\n\",\n",
    "        \"    print(f\\\"P-value: {p_value:.6f}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return p_value, suspect_scores, validation_scores\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 4. Data Preparation\\n\",\n",
    "        \"\\n\",\n",
    "        \"Here, we set up our mock datasets. \\n\",\n",
    "        \"- The `suspect_member_dataset` simulates data that the LLM *was* trained on.\\n\",\n",
    "        \"- The `known_non_member_dataset` simulates data that the LLM has *never* seen.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# This dataset simulates data that the LLM *was* trained on.\\n\",\n",
    "        \"# The fine-tuning process will cause a \\\"recovery\\\" of forgotten knowledge,\\n\",\n",
    "        \"# leading to a large output shift (and thus low similarity scores).\\n\",\n",
    "        \"suspect_member_dataset = {\\n\",\n",
    "        \"    'train': [{'prompt': f'From the old book, chapter {i}:'} for i in range(50)],\\n\",\n",
    "        \"    'test': [{'prompt': f'From the old book, chapter {i+50}:'} for i in range(20)]\\n\",\n",
    "        \"}\\n\",\n",
    "        \"\\n\",\n",
    "        \"# This dataset simulates data that the LLM was *not* trained on.\\n\",\n",
    "        \"# The fine-tuning process introduces entirely new knowledge,\\n\",\n",
    "        \"# leading to a smaller, more moderate output shift.\\n\",\n",
    "        \"known_non_member_dataset = {\\n\",\n",
    "        \"    'train': [{'prompt': f'From the new article, section {i}:'} for i in range(50)],\\n\",\n",
    "        \"    'test': [{'prompt': f'From the new article, section {i+50}:'} for i in range(20)]\\n\",\n",
    "        \"}\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 5. Execution and Interpretation\\n\",\n",
    "        \"\\n\",\n",
    "        \"Now, we run the full analysis and interpret the results based on the p-value.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Load the models (conceptually)\\n\",\n",
    "        \"base_llm, similarity_scorer = get_model_and_scorer()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Run the analysis\\n\",\n",
    "        \"p_value_result, suspect_scores, validation_scores = run_catshift_analysis(\\n\",\n",
    "        \"    base_llm, \\n\",\n",
    "        \"    similarity_scorer, \\n\",\n",
    "        \"    suspect_member_dataset, \\n\",\n",
    "        \"    known_non_member_dataset\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# --- INTERPRETATION ---\\n\",\n",
    "        \"# The paper uses a significance level (alpha) of 0.1\\n\",\n",
    "        \"alpha = 0.1\\n\",\n",
    "        \"print(\\\"\\\\n--- FINAL CONCLUSION ---\\\")\\n\",\n",
    "        \"if p_value_result < alpha:\\n\",\n",
    "        \"    print(f\\\"Result: P-value ({p_value_result:.6f}) is less than alpha ({alpha}).\\\")\\n\",\n",
    "        \"    print(\\\"Conclusion: We REJECT the null hypothesis.\\\")\\n\",\n",
    "        \"    print(\\\"This suggests the suspect dataset's output shift is SIGNIFICANTLY DIFFERENT from the non-member baseline.\\\")\\n\",\n",
    "        \"    print(\\\"Therefore, the suspect dataset is LIKELY a MEMBER of the original training data.\\\")\\n\",\n",
    "        \"else:\\n\",\n",
    "        \"    print(f\\\"Result: P-value ({p_value_result:.6f}) is not less than alpha ({alpha}).\\\")\\n\",\n",
    "        \"    print(\\\"Conclusion: We FAIL to reject the null hypothesis.\\\")\\n\",\n",
    "        \"    print(\\\"There is no strong statistical evidence to distinguish the suspect dataset from a non-member.\\\")\\n\",\n",
    "        \"    print(\\\"Therefore, the suspect dataset is LIKELY NOT a member of the original training data.\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 6. Visualization\\n\",\n",
    "        \"\\n\",\n",
    "        \"A key part of data analysis is visualization. Let's plot histograms of the two similarity score distributions. We expect to see a clear separation between the two.\\n\",\n",
    "        \"\\n\",\n",
    "        \"- **Validation Scores (Blue)**: Should be higher, indicating less change after fine-tuning.\\n\",\n",
    "        \"- **Suspect Scores (Red)**: Should be lower, indicating a significant change (a \\\"shift\\\") as the model \\\"recovers\\\" forgotten knowledge.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"plt.figure(figsize=(10, 6))\\n\",\n",
    "        \"plt.hist(validation_scores, bins=15, alpha=0.7, label='Validation (Non-Member) Scores', color='blue', density=True)\\n\",\n",
    "        \"plt.hist(suspect_scores, bins=15, alpha=0.7, label='Suspect (Member) Scores', color='red', density=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.title('Distribution of Similarity Scores Before vs. After Fine-Tuning')\\n\",\n",
    "        \"plt.xlabel('Similarity Score (Lower score = bigger output shift)')\\n\",\n",
    "        \"plt.ylabel('Density')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(axis='y', linestyle='--', alpha=0.7)\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"language\": \"python\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"codemirror_mode\": {\n",
    "        \"name\": \"ipython\",\n",
    "        \"version\": 3\n",
    "      },\n",
    "      \"file_extension\": \".py\",\n",
    "      \"mimetype\": \"text/x-python\",\n",
    "      \"name\": \"python\",\n",
    "      \"nbconvert_exporter\": \"python\",\n",
    "      \"pygments_lexer\": \"ipython3\",\n",
    "      \"version\": \"3.9.7\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 2\n",
    "}\n"
   ],
   "id": "6f1f4478c97f61db"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
